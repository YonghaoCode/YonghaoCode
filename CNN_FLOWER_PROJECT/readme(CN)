使用MobileNetV2进行迁移学习和构建CNN

项目目标：
本项目旨在使用Oxford 102 Flower数据集，通过卷积神经网络（CNN）对花卉图像进行分类。从实现自定义CNN开始，逐步引入MobileNetV2的迁移学习，并通过微调优化模型性能。在此过程中，我们探索了深度学习的关键定义和规律，加深了理解。

关键步骤与模型优化：

数据预处理：

操作： 加载并预处理Oxford 102 Flower数据集，将图像大小调整为224x224（与MobileNetV2输入兼容）并归一化。将数据集划分为训练集和测试集。
优化： 确保图像尺寸一致，同时通过数据增强（如缩放和归一化）提高泛化能力。
自定义CNN实现：

操作： 构建包含Conv2D、ReLU、MaxPooling、Flatten和全连接层的CNN架构。使用CrossEntropyLoss作为损失函数，Adam优化器进行训练。
不足： 尽管在训练集上表现良好，但测试集准确率提升有限，表明需要进一步改进。
收获： 自定义CNN是理解基础的重要步骤，但对数据量和参数调整要求较高。
基于MobileNetV2的迁移学习：

操作： 使用预训练的MobileNetV2模型，替换分类器层为自定义全连接层，并冻结所有预训练层，仅训练新添加的分类层。
优化： 通过迁移学习，在较少训练轮数内显著提高了准确率。
收获： 迁移学习适用于小规模数据集，能有效减少训练时间并提高性能。
模型微调：

操作： 解冻MobileNetV2的深层卷积层，降低学习率，并增加训练轮数以进一步调整模型。
优化： 测试集准确率从约57%提升至79%。微调使高层特征更适应目标数据集。
收获： 选择性解冻层和超参数的精细调整是迁移学习优化的关键。
性能优化：

操作： 调整学习率、批量大小、训练轮数等超参数。引入学习率调度器（StepLR）以动态调整学习率。
优化： 减少过拟合，提高收敛速度，实现测试集损失和准确率的平衡。
今日收获的定义和规律：

批量（Batch）： 数据集的一个子集，用于一次正向和反向传播。较大批量稳定梯度更新但需更多内存，较小批量增加噪声但有助于正则化。

轮次（Epoch）： 数据集的完整遍历。多轮次训练允许模型通过迭代不断优化参数。

迁移学习（Transfer Learning）： 复用预训练模型完成新任务，大幅减少训练时间，适用于特征分布相似的源域和目标域。

学习率（Learning Rate）： 决定权重更新的步长。微调中较小的学习率有助于模型更稳定地收敛。

微调（Fine-Tuning）： 通过解冻特定层并重新训练模型，结合迁移学习效率和任务适应性。

MobileNetV2的深度可分离卷积： 将空间和通道操作分离，大幅减少计算量，非常适合移动设备。

总结：

从自定义CNN开始，逐步过渡到迁移学习并实现性能提升。
每一步优化都基于上一阶段的成果，通过实践巩固理论。
最终实现了**79.04%**的测试集准确率，验证了深度学习中逐步迭代的重要性。
最终感想：
深度学习的工作流程具有迭代性。每个阶段（预处理、架构设计、迁移学习、微调）相辅相成，将理论与实践结合，最终实现高性能的模型。
